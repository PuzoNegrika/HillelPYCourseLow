# Веб-відкриття: Підводження підсумків скрапінгу та збереження даних.
# Що таке скрапінг даних з сайтів.
# !!! Скрапінг даних з сайтів - це процес автоматичного збирання даних із веб-сайтів із використанням спеціальних програмних інструментів
# Він дозволяє користувачеві витягувати інформацію з веб-сторінок та зберігати її у зручному для подальшого аналізу форматі
# https://uk.wikipedia.org/wiki/Web_scraping + є фото
#
# Скрапінг даних з сайтів може мати різні цілі та застосування, наприклад:  !!!
# 1 - Збір даних про продукти, ціни, відгуки, контакти тощо з різних веб-сайтів електронної комерції, соціальних мереж, каталогів тощо
# 2 - Збір даних для наукових, журналістських, освітніх, конкурентних або ринкових досліджень
# 3 - Збір даних для побудови пошукових індексів, баз даних, аналітичних панелей, візуалізацій тощо
#
# Скрапінг даних з сайтів може використовувати різні технології та методи, наприклад:  !!!
# 1 - Веб-скрейпери - це програми, які імітують поведінку людини в інтернеті, відкриваючи веб-сторінки, знаходячи потрібні елементи, витягуючи дані, зберігаючи або надсилаючи їх до іншого місця
# 2 - Веб-краулери - це програми, які автоматично переходять по посиланнях на веб-сторінках, збираючи дані з різних джерел, створюючи копії або індекси веб-сайтів
# 3 - API (Application Programming Interface) - це інтерфейси, які дозволяють отримувати дані з веб-сайтів у структурованому форматі, використовуючи запити та відповіді
# 4 - Парсери - це програми, які аналізують структуру веб-сторінок, використовуючи мови розмітки, такі як HTML, XML, JSON тощо, і вилучають потрібні дані з них

# Скрапінг даних з сайтів може мати різні складності та обмеження, залежно від джерела даних, обсягу даних, частоти збирання, якості даних, правових питань тощо
# Тому важливо вибрати найкращий метод та інструмент для скрапінгу даних з сайтів, враховуючи ці фактори


# Які бувають сайти, з точкі зору створення контенту.
# !!! Для відображення контенту будь-якого сайту в браузері необхідно, щоб браузер отримав html-документ
# !!! HTML (HyperText Markup Language) - це мова розмітки, яка використовується для створення структури та зовнішнього вигляду веб-сторінок
#  HTML дозволяє використовувати різні елементи, такі як теги, атрибути, коментарі, списки, таблиці, посилання, зображення, форми, відео тощо, для відображення контенту на сайті
#
# З погляду формування HTML, сайти можна поділити на такі основні види:  !!!
# 1 - Статичні сайти - це сайти, які складаються з одного або декількох файлів HTML, які не змінюються під час роботи сайту
# Ці сайти показують однаковий контент усім відвідувачам, і не вимагають взаємодії з сервером або базою даних
# Статичні сайти зазвичай швидко завантажуються, легко розробляються та підтримуються, але мають обмежені можливості для динамічного та
# інтерактивного контенту. Приклади статичних сайтів: Сайт-візитка, Landing Page, Персональний сайт.
# 2 - Динамічні сайти - це сайти, які генерують контент на льоту за допомогою скриптів на стороні сервера або клієнта
# Ці сайти показують різний контент різним відвідувачам, залежно від їхніх запитів, дій, налаштувань, даних тощо
# Динамічні сайти вимагають взаємодії з сервером або базою даних, що дозволяє зберігати, обробляти та відображати інформацію
# Динамічні сайти зазвичай мають більше можливостей для динамічного та інтерактивного контенту, але вони складніші в розробці та підтримці,
# а також можуть бути повільнішими в завантаженні. Приклади динамічних сайтів: Інтернет-магазин, Соціальна мережа, Блог

# Існують також інші види сайтів, які можуть поєднувати різні методи генерації контенту або мати специфічні особливості
# Наприклад, [WEB-додаток] - це сайт, який надає функціональність, схожу на програмне забезпечення, і використовує скрипти на стороні клієнта для створення динамічного інтерфейсу
# HTML документ може формуватись як на стороні сервера, так і засобами фреймворків у самому браузері

# !!! Різниця між сайтами, що формують html на сервері або за допомогою JavaScript фреймворку, полягає в тому, де відбувається генерація html-коду для веб-сторінок
# 1 - Сайти, що формують html на сервері, використовують скриптові мови, такі як PHP, Python, Ruby on Rails тощо, для створення динамічного html-коду на основі запитів користувачів, даних з бази даних, логіки бізнесу та інших факторів
# !!! Цей код потім відправляється до браузера користувача, де він відображається як веб-сторінка. Такі сайти називаються серверно-орієнтованими
# 2 - Сайти, що формують html за допомогою JavaScript фреймворку, використовують бібліотеки та інструменти JavaScript, такі як React, Angular, Vue тощо, для створення динамічного html-коду на стороні клієнта, тобто в браузері користувача
# !!! Цей код залежить від даних, які отримуються з сервера за допомогою AJAX-запитів, а також від дій користувача, стану додатку, маршрутизації та інших факторів. Такі сайти називаються клієнт-орієнтованими

# Основні переваги сайтів, що формують html на сервері, полягають у тому, що вони:  !!!
# 1 - забезпечують кращу сумісність з різними браузерами, оскільки не вимагають підтримки JavaScript
# 2 - забезпечують кращу оптимізацію для пошукових систем, оскільки вони надсилають повний html-код, який може бути проіндексований
# 3 - забезпечують кращу безпеку, оскільки вони не відкривають деталі логіки бізнесу та даних на стороні клієнта
# Основні переваги сайтів, що формують html за допомогою JavaScript фреймворку, полягають у тому, що вони:  !!!
# 1 - забезпечують кращу швидкість та відгук, оскільки вони не вимагають повного перезавантаження сторінки при кожному запиті
# 2 - забезпечують кращу інтерактивність та користувацький досвід, оскільки вони дозволяють створювати сучасні та адаптивні інтерфейси
# 3 - забезпечують кращу масштабованість та гнучкість, оскільки вони дозволяють використовувати один і той же код для різних платформ та пристроїв


# Емуляція роботи браузера за допомогою selenium.
# З вищеописаного випливає - для того, щоб html документ міг бути сформований за допомогою JavaScript фреймворку, необхідна наявність браузера, щоб цей фреймворк міг виконувати свої функції.
# !!! Емуляція роботи браузера за допомогою selenium в Python відбувається наступним чином:  !!!
# 1 - Selenium - це бібліотека Python, яка дозволяє автоматизувати веб-браузери, такі як Google Chrome, Firefox, Internet Explorer тощо:
# https://selenium-python.readthedocs.io/
# 2 - Для емуляції роботи браузера потрібно встановити selenium за допомогою команди pip install selenium
# 3 - Також потрібно завантажити та розмістити відповідний драйвер для браузера, який хочете емулювати, наприклад:
# chromedriver для Google Chrome або Firefox: https://selenium-python.readthedocs.io/installation.html#drivers
# 4 - Потім потрібно імпортувати selenium в свій Python-скрипт та створити об'єкт webdriver, який представляє браузер
# 5 - За допомогою об'єкта webdriver можна виконувати різні дії з браузером, такі як відкривати веб-сторінки, заповнювати форми,
# клікати на елементи, робити скріншоти тощо
# 6 - Для знаходження елементів на веб-сторінці можна використовувати різні методи, такі як find_element_by_id, find_element_by_name,
# find_element_by_xpath тощо
# 7 - Для взаємодії з елементами можна використовувати різні методи, такі як click, send_keys, clear, submit тощо
# 8 - Для закриття браузера можна використовувати метод quit, який закриє усі вікна та видалить об'єкт webdriver

# !!! Ось приклад Python-скрипту, який емулює роботу браузера Google Chrome та відкриває веб-сторінку Bing:  !!!
# import ti
# import time
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
#
# # Створюємо об'єкт webdriver для Google Chrome
# service = Service(executable_path=r'C:\Users\user\PycharmProjects\chromedriver.exe')
# driver = webdriver.Chrome(service=service)
# # Відкриваємо веб-сторінку Bing
# driver.get("https://www.bing.com")
# time.sleep(1) # Потрібно дочекатися формування сторінки
#
# # Знаходимо поле пошуку за ідентифікатором
# search_box = driver.find_element_by_id("sb_form_q")
#
# # Вводимо запит "selenium python" в поле пошуку
# search_box.send_keys("selenium python")
#
# # Натискаємо кнопку пошуку за іменем
# search_button = driver.find_element_by_name("go")
# search_button.click()
# # Пауза, щоб побачити результати пошуку
# time.sleep(3)
#
# # Закриваємо браузер
# driver.quit()


# requests - робота з інтернет-запитами.
# !!! Якщо html документ формується на сервері, браузер нам не потрібен. Достатньо зробити запит на сервер на потрібну адресу, щоб отримати готовий html документ
# !!! Для того, щоб спілкуватися з сервером з Python-скрипту, можна використовувати бібліотеку requests
#
# Для роботи з інтернет-запитами за допомогою requests в Python потрібно виконати наступні кроки:  !!!
# 1 - Встановити бібліотеку requests за допомогою команди pip install requests
# 2 - Імпортувати бібліотеку requests в свій Python-скрипт за допомогою команди import requests
# 3 - Використовувати методи requests, такі як get, post, put, delete тощо, для виконання запитів до веб-сайтів
# Ці методи приймають URL-адресу як перший параметр та інші необов'язкові параметри, такі як params, data.csv, headers, cookies тощо,
# для налаштування запиту
# 4 - Отримувати об'єкт відповіді, який містить інформацію про статус, заголовки, тіло, історію, час тощо відповіді
# 5 - Використовувати атрибути та методи об'єкта відповіді, такі як status_code, text, json, content, raise_for_status тощо,
# для отримання та обробки даних з відповіді.

# Ось приклад Python-скрипту, який виконує GET-запит до веб-сайту Bing та виводить текст відповіді:  !!!
import requests

# Виконуємо GET-запит до веб-сайту Bing
response = requests.get("https://www.bing.com")

# Перевіряємо, чи запит був успішним
if response.status_code == 200:
    # Виводимо текст html документу
    ## print(response.text) - дуже багато тексту зі сторінки
    pass


# Beautiful Soup - аналіз HTML та XML документів.
# !!! Коли ми вже маємо готовий html документ, нам необхідно знайти інструмент, який дозволить витягти потрібні нам дані з нього. У цьому випадку нам допоможе бібліотека Beautiful Soup
# !!! Beautiful Soup - це бібліотека Python, яка дозволяє аналізувати HTML і XML документи, витягувати дані з них, модифікувати їх або створювати нові https://realpython.com/beautiful-soup-web-scraper-python/
#
# Для роботи з Beautiful Soup в Python потрібно виконати наступні кроки:  !!!
# 1 - Встановити бібліотеку Beautiful Soup за допомогою команди pip install beautifulsoup4
# 2 - Імпортувати бібліотеку Beautiful Soup в свій Python-скрипт за допомогою команди from bs4 import BeautifulSoup
# 3 - Створити об'єкт Beautiful Soup, який представляє документ, який потрібно аналізувати. Це можна зробити за допомогою конструктора
# BeautifulSoup(html, parser), де html - це рядок або файл, який містить HTML або XML код, а parser - це назва парсера, який використовується
# для аналізу документу, наприклад, html.parser, lxml, xml тощо
# 4 - Використовувати атрибути та методи об'єкта Beautiful Soup, такі як find, find_all, select, select_one, get_text, get_attribute,
# append, insert, replace_with, decompose тощо, для пошуку, вилучення, зміни або видалення елементів документу

# Ось приклад Python-скрипту, який аналізує HTML-документ, знаходить усі теги <p> та виводить їх текстовий вміст:  !!!
from bs4 import BeautifulSoup

# Створюємо об'єкт Beautiful Soup з HTML-рядка
html = """
<html><body><h1>Заголовок</h1>
<p>Перший абзац</p>
<p>Другий абзац</p>
</body></html>
"""
soup = BeautifulSoup(html, "html.parser")

# Знаходимо усі теги <p>
paragraphs = soup.find_all("p")

# Виводимо текстовий вміст кожного тега <p>
for p in paragraphs:
    print(p.get_text())
    # Вивід:
    # Перший абзац
    # Другий абзац
print()


# Парсинг даних на сторінці HTML-документа.
# Коли ми створили об'єкт Beautiful Soup з HTML-документа, ми можемо почати пошук необхідних нам даних
# Для цього необхідно проаналізувати HTML-теги, з яких складається документ, і в яких є потрібна нам інформація
# !!! А також потрібно знайти закономірності в назвах css-класів для тих тегів, всередині яких знаходиться інформація, яку ми шукаємо
# !!! Далі, за допомогою методів Beautiful Soup, нам необхідно знайти та опрацювати всю доступну інформацію
#
# Приступимо до практики. Створимо кілька функцій, які оброблятимуть звернення до інтернет магазину:  !!!
# import time
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.firefox.service import Service
#
# def get_data_by_selenium(url: str) -> str:
#     """ Звертається до сервера за url адресою і повертає HTML сайту"""
#     service = Service(r"C:\Users\admin\PyCharmMiscProject\16HillelLesson\chromedriver.exe")
#     driver = webdriver.Chrome(service=service)
#     driver.get(url)
#     time.sleep(5)
#     html = driver.page_source
#     driver.quit()
#     return html
#
# def parse_data(data.csv: str) -> list:
#     """ Функція парсингу даних з хтмл документа"""
#     soup = BeautifulSoup(data.csv, "html.parser")
#     items = soup.select(".goods-tile__title")
#     return [i.get_text(strip=True) for i in items]
#
# def main() -> None:
#     """ Головна функція диригент"""
#     url = "https://hard.rozetka.com.ua/videocards/c80087/"
#     data.csv = get_data_by_selenium(url)
#     rows = parse_data(data.csv)
#     print(rows)
#
# if __name__ == '__main__':
#     main()

# Поки що цей код тільки відкриває сторінку з відеокартами і більше нічого не робить. Потрібно проаналізувати, де знаходяться дані в документі
# Для того, щоб мати можливість аналізувати документ хтмл, необхідно включити інструмент розробника
# Після детального аналізу, можна побачити, що список всіх відеокарт на цій сторінці знаходиться всередині тега li, css-клас якого catalog-grid__cell
# Почнемо модифікувати функцію парсингу даних:
# Створимо об'єкт BeautifulSoup і знайдемо всі елементи li, які є на сторінці і які мають потрібний нам клас catalog-grid__cell:
# def parse_data(data.csv: str) -> list:
#     """ Функція парсингу даних з хтмл документа"""
#     rez = []
#     if data.csv:
#         soup = BeautifulSoup(data.csv, 'html.parser')
#         li_list = soup.find_all('li', attrs={'class': 'catalog-grid__cell'})
#     return rez
# Після цих дій, у списку li_list буде набір екземплярів класу BeautifulSoup, з якими можна працювати окремо так само, як і з основним документом
# Проаналізуємо інформацію для кожного елемента списку, для пошуку розташування потрібної нам інформації
# Інтернет посилання та опис товару знаходяться у тезі а, з класом goods-tile__heading
# Стара ціна знаходиться в контейнері div з класом goods-tile__price - old, поточна ціна - в контейнері div з класом goods-tile__price
# Обійдемо всі елементи списку в циклі та обробимо кожен із них, за для отримання необхідної інформації
# def parse_data(data.csv: str) -> list:
#     """ Функція парсингу даних з хтмл документа"""
#     rez = []
#     if data.csv:
#         soup = BeautifulSoup(data.csv, 'html.parser')
#         li_list = soup.find_all('li', attrs={'class': 'catalog-grid__cell'})
#         for li in li_list:
#             # Пошук тега а
#             a = li.find('a', attrs={'class': 'goods-tile__heading'})
#             # Беремо у тега а атрибут href
#             href = a['href']
#             # За допомогою атрибуту текст, забираємо всю текстову
#             # інформацію, що міститься в цьому тегу
#             title = a.text
#             old = li.find('div', attrs={'class': 'goods-tile__price--old'})
#             price = li.find('div', attrs={'class': 'goods-tile__price'})
#             # Стара ціна є не у всіх, тому потрібно зробити дефолтне значення
#             old_price = ''
#             if old:
#                 # Якщо контейнер із старою ціною є
#                 old = old.text
#                 # І в цьому контейнер є інфа
#                 if old:
#                     # Забираємо лише те, що є цифрами та формуємо значення ціни
#                     old_price = int(''.join(c for c in old if c.isdigit()))
#             # Звичайна ціна є скрізь, тому формуємо значення
#             price = int(''.join(c for c in price.text if c.isdigit()))
#             # Результат за кожною відеокартою записуємо у вигляді словника
#             rez.append({
#                 'title': title, 'href': href, 'price': price, 'old_price': old_price
#             })
#     return rez

# Таким чином, після закінчення роботи цієї функції, ми отримаємо набір даних який вже можна десь зберігати


# Збереження даних у csv-файл.
# !!! Модуль csv в Пайтон - це вбудований модуль, який дозволяє читати та записувати дані у форматі CSV (Comma-Separated Values), який є найпоширенішим форматом для обміну табличними даними між різними програмами
# !!! Модуль csv надає класи для роботи з файлами або рядками, які містять дані у форматі CSV, такі як reader, writer, DictReader, DictWriter тощо
#
# За допомогою цих класів можна виконувати такі операції, як:  !!!
# 1 - Читання даних з CSV-файлів або рядків у вигляді списків або словників
# 2 - Запис даних у CSV-файли або рядки зі списків або словників
# 3 - Налаштування параметрів форматування CSV, таких як роздільник, символ лапок, режим цитування тощо
# 4 - Обробка помилок та винятків, пов'язаних з CSV-даними

# Коли йдеться про роботу з CSV-файлами, на думку може спасти використання простого читання та запису рядків з використанням стандартних функцій Python
# Однак формат CSV може бути набагато складнішим, ніж здається на перший погляд. Різні діалекти CSV можуть використовувати різні символи-розділювачі (не тільки коми), а рядки можуть містити лапки, нові рядки та інші спеціальні символи, які потрібно правильно інтерпретувати або екранувати
# Саме тут модуль CSV приходить на допомогу. Він надає функції та класи, які автоматизують більшу частину складнощів читання та запису стандартизованих CSV-файлів

# Застосування модуля:  !!!
# 1 - Читання та запис: За допомогою функцій csv.reader() і csv.writer(), ви можете легко читати і записувати в CSV-файли,
# Для словників існують функції csv.DictReader() та csv.DictWriter()
# 2 - Діалекти та налаштування: CSV не має суворого стандарту, і різні системи можуть використовувати різні конвенції.
# Модуль CSV дозволяє визначити "діалект", який описує різні характеристики формату, такі як символ роздільника, лапки і т.д
# 3 - Обробка винятків: Під час роботи з даними завжди можливі помилки. Модуль csv включає специфічні винятки, такі як csv.Error,
# щоб допомогти вам ловити і обробляти помилки, пов'язані з даними CSV

# Читання даних із CSV-файлів
# Використання csv.reader() csv.reader() — це функція, яка читає файл CSV і повертає ітератор рядків. Кожен рядок представлений у вигляді списку
# Припустимо, у вас є файл data.csv.csv з таким вмістом:
# Ім'я,Вік,Професія
# Аліса, 28, Інженер
# Боб,22,Датасаентист
# Чарлі,35,Дизайнер
# Щоб прочитати цей файл, ви можете використовувати наступний код:
import csv

with open('data.csv', 'r', encoding='utf-8') as file:
    reader = csv.reader(file)
    for row in reader:
        print(row)

# Цей код виведе:
# ["Ім'я", 'Вік', 'Професія']
# ['Аліса', '28', 'Інженер']
# ['Боб', '22', 'Датасаентист']
# ['Чарлі', '35', 'Дизайнер']

# Використання DictReader для читання даних у словнику
# csv.DictReader() читає CSV-файл і повертає кожен рядок у вигляді словника, де ключі словника беруться із заголовка CSV-файлу. Приклад:
with open('data.csv', 'r', encoding='utf-8') as file:
    reader = csv.DictReader(file)
    for row in reader:
        print(row)
print()

# Цей код виведе:
# {"Ім'я": "Аліса", "Вік": "28", "Професія": "Інженер"}
# {"Ім'я": "Боб", "Вік": "22", "Професія": "Датасаентист"}
# {"Ім'я": "Чарлі", "Вік": "35", "Професія": "Дизайнер"}

# Запис даних у CSV-файли
# Доопрацюємо наш код таким чином, щоб можна було зберегти отримані дані у файл
# Створимо функцію для збереження даних у csv-файл:
# def save_to_csv(rows) -> None:
#     """Функція збереження даних у csv-файл"""
#     csv_title = ['title', 'href', 'price', 'old_price', ]
#     with open('videocards.csv', 'w') as f:
#         writer = csv.DictWriter(f, fieldnames=csv_title, delimiter=';')
#         writer.writeheader()
#         writer.writerows(rows)
#
# # І додамо виклик цієї функції всередині функції main:
# def main() -> None:
#     """ Головна функція диригент"""
#     url = 'https://hard.rozetka.com.ua/videocards/c80087/'
#     data = get_data_by_selenium(url)
#     rows = parse_data(data)
#     save_to_csv(rows)


# Збереження даних у БД (SQLite).
# SQLite в Пайтон - це спосіб роботи з легкою та вбудованою базою даних, яка не потребує окремого сервера або конфігурації
# SQLite використовує файл для зберігання даних у форматі SQL, який можна легко переносити, копіювати або резервувати
# !!! SQLite підтримує більшість стандартних функцій SQL, таких як створення, зміна, запитання, видалення таблиць та записів, а також транзакції, індекси, підзапити, об'єднання тощо

# Для роботи з SQLite в Пайтон потрібно використовувати модуль sqlite3, який є частиною стандартної бібліотеки Пайтон і сумісний зі специфікацією DB-API 2.0
# Модуль sqlite3 надає класи та функції для виконання операцій з базою даних SQLite, такі як:  !!!
# 1 - connect - функція, яка створює або відкриває з'єднання з файлом бази даних SQLite
# 2 - Connection - клас, який представляє з'єднання з базою даних SQLite і дозволяє виконувати команди SQL, створювати курсори,
# керувати транзакціями, отримувати інформацію про базу даних тощо
# 3 - Cursor - клас, який представляє курсор, який дозволяє виконувати запити SQL, отримувати результати, пересуватися по записах,
# вилучати метадані тощо
# 4 - Row - клас, який представляє рядок результату запиту SQL і дозволяє отримувати значення за індексом або іменем стовпця
# 5 - adapt та register_adapter - функції, які дозволяють адаптувати об'єкти Пайтон до типів даних SQLite
# 6 - convert та register_converter - функції, які дозволяють конвертувати типи даних SQLite в об'єкти Пайтон

# Ось приклад Python-скрипту, який створює базу даних SQLite, додає таблицю з даними про книги, робить запит до таблиці та виводить результати:
import sqlite3
import os

db_file = "books.db"

# Перевіряємо, чи існує файл бази даних
db_exists = os.path.exists(db_file)

# Створюємо з'єднання з файлом бази даних SQLite
conn = sqlite3.connect("books.db")

# Створюємо курсор для виконання команд SQL
cur = conn.cursor()

# Створюємо таблицю та вставляємо записи лише якщо база ще не існує
if not db_exists:
    # Створюємо таблицю books з чотирма стовпцями
    cur.execute("CREATE TABLE books (id INTEGER PRIMARY KEY, title TEXT, author TEXT, year INTEGER)")

    # Додаємо три записи до таблиці books
    cur.execute("INSERT INTO books VALUES (1, 'Гайдамаки', 'Тарас Шевченко', 1841)")
    cur.execute("INSERT INTO books VALUES (2, 'Гаррі Поттер і філософський камінь', 'Джоан Роулінг', 1997)")
    cur.execute("INSERT INTO books VALUES (3, 'Шерлок Холмс', 'Артур Конан Дойл', 1892)")
    # Зберігаємо зміни в базі даних
    conn.commit()

# Робимо запит до таблиці books, щоб отримати усі книги, написані після 1900 року
cur.execute("SELECT * FROM books WHERE year > 1900")

# Отримуємо усі рядки результату запиту
rows = cur.fetchall()

# Виводимо кожен рядок на екран
for row in rows:
    print(row)

# Закриваємо з'єднання з базою даних
conn.close()


# Збереження даних у БД.
# Для того, щоб зберігати дані з відеокарт у базі даних, нам необхідно модифікувати наш код
# По-перше, створимо функції, через які ми записуватимемо дані в БД. Перша створює таблицю video_cards, якщо такої ще немає. Друга - підключається до бази даних, і записує дані у обрану таблицю:
# import sqlite3
#
# def create_table() -> bool:
#     """створює таблицю video_cards, якщо такої ще немає"""
#     sqlite_connection = sqlite3.connect('cards.db')
#     sqlite_create_table_query = '''
#             CREATE TABLE IF NOT EXISTS video_cards (
#                 id INTEGER PRIMARY KEY,
#                 title TEXT NOT NULL,
#                 url TEXT NOT NULL,
#                 price INTEGER NOT NULL,
#                 old_price INTEGER NULL
#             );'''
#     cursor = sqlite_connection.cursor()
#     print("База даних підключена до SQLite")
#     cursor.execute(sqlite_create_table_query)
#     sqlite_connection.commit()
#     return True
#
#
# def save_to_db(rows) -> None:
#     """підключається до бази даних, і записує дані у таблицю video_cards"""
#     if create_table():
#         sqlite_connection = sqlite3.connect('cards.db')
#         cursor = sqlite_connection.cursor()
#         cursor.executemany(
#             "INSERT INTO video_cards('title', 'url', 'price', 'old_price' ) VALUES (?,?,?,?)",
#             rows)
#         sqlite_connection.commit()

# Після цього нам необхідно модифікувати дані для запису - замість списку словників нам необхідний список кортежів:
# def parse_data(data: str) -> list:
#     """ Функція парсингу даних з хтмл документа"""
#     rez = []
#     if data:
#         soup = BeautifulSoup(data, 'html.parser')
#         li_list = soup.find_all('li', attrs={'class': 'catalog-grid__cell'})
#         for li in li_list:
#             # Пошук тега а
#             a = li.find('a', attrs={'class': 'goods-tile__heading'})
#             # Беремо у тега а атрибут href
#             href = a['href']
#             # За допомогою атрибуту текст, забираємо всю текстову
#             # інформацію, що міститься в цьому тегу
#             title = a.text
#             old = li.find('div', attrs={'class': 'goods-tile__price--old'})
#             price = li.find('div', attrs={'class': 'goods-tile__price'})
#             # Стара ціна є не у всіх, тому потрібно зробити дефолтне значення
#             old_price = ''
#             if old:
#                 # Якщо контейнер із старою ціною є
#                 old = old.text
#                 # І в цьому контейнер є інфа
#                 if old:
#                     # Забираємо лише те, що є цифрами та формуємо значення ціни
#                     old_price = int(''.join(c for c in old if c.isdigit()))
#             # Звичайна ціна є скрізь, тому формуємо значення
#             price = int(''.join(c for c in price.text if c.isdigit()))
#             # Результат за кожною відеокартою записуємо у вигляді словника
#             # rez.append({
#             #     'title': title, 'href': href, 'price': price,
#             #     'old_price': old_price
#             # })
#             # Для збереження в БД, нам потрібний список кортежів.
#             rez.append((title, href, price, old_price))
#     return rez

# І змінимо виклик функції збереження даних - замість збереження в csv-файл, викликатимемо функцію збереження в БД:
# def main() -> None:
#     """ Головна функція диригент"""
#     url = 'https://hard.rozetka.com.ua/videocards/c80087/'
#     data = get_data_by_selenium(url)
#     rows = parse_data(data)
#     # save_to_csv(rows)
#     save_to_db(rows)

# Поруч із файлом main.py з'явився файл cards.db, який і є нашою базою даних
# Відкрити базу даних на основі sqlite3 можна за допомогою спеціальних програм. Наприклад: https://sqlitebrowser.org/
